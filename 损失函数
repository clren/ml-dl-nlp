

损失函数（loss function）：计算的是一个样本的误差。它是用来估量你模型的预测值 f(x)与真实值 Y的不一致程度，通常用 L(Y,f(x))来表示。
代价函数（cost function）：是整个训练集上所有样本误差的平均。本质上看，和损失函数是同一个东西。
目标函数：代价函数 + 正则化项。

作者：高永峰_GYF
链接：https://www.jianshu.com/p/b5b09ce54a22
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。





Log Loss 在使用似然函数最大化时，其形式是进行连乘，但是为了便于处理，一般会套上log，这样便可以将连乘转化为求和，由于log函数是单调递增函数，因此不会改变优化结果








常用损失函数小结

置顶 视觉一只白 2018-05-27 11:01:58 

 41230 

 收藏 74
展开
一、摘要
本文主要总结一下常见的损失函数，包括：MSE均方误差损失函数、SVM合页损失函数、Cross Entropy交叉熵损失函数、目标检测中常用的Smooth L1损失函数。其中还会涉及到梯度消失、梯度爆炸等问题：ESM均方误差+Sigmoid激活函数会导致学习缓慢；Smooth L1损失是为了解决梯度爆炸问题。仅供参考。
二、均方误差损失
2.1 均方差损失函数的定义：
均方差损失函数常用在最小二乘法中。它的思想是使得各个训练点到最优拟合线的距离最小（平方和最小）。均方差损失函数也是我们最常见的损失函数了，相信大很熟悉了，我们以神经网络中激活函数的形式表达一下，定义如下：

其中， 
：x是输入、w和b是网络的参数、 
是激活函数。
2.2 ESM均方误差+Sigmoid激活函数：输出层神经元学习率缓慢
2.2.1 Sigmoid激活函数：
这个激活函数再熟悉不过了，该函数能将负无穷到正无穷的数映射到0和1之间。先来看一下表达式以及函数图像：

Sigmoid的导数推导以及图像：



从sigmiod的导数图像中可以看到，除了中间比较小的区域，其他区域的十分值接近于0。
神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差（网络输出和标签之间的偏差）因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重w没有更新，即梯度消失。可以看出，sigmoid函数作为激活函数本身就存在梯度消失的问题。
2.2.2 ESM均方误差+Sigmoid激活函数：输出层神经元学习率缓慢
我们以一个神经元，ESM均方误差损失 
，Sigmoid激活函数 
(其中 
)为例，计算一下最后一层的反向传播过程，可得：



可以看到最后一层反向传播时，所求的梯度中都含有 
。经过上面的分析，当神经元输出接近1时候，Sigmoid的导数 
变很小，这样 
、 
很小，这就导致了ESM均方误差+Sigmoid激活函数使得神经网络反向传播的起始位置——输出层神经元学习率缓慢。
想要解决这个问题，需要引入接下来介绍的交叉熵损失函数。这里先给出结论：交叉熵损失+Sigmoid激活函数可以解决输出层神经元学习率缓慢的问题，但是不能解决隐藏层神经元学习率缓慢的问题。具体的推导和总结在下面部分中介绍。
三、交叉熵损失
3.1 交叉损失的定义
交叉熵损失的计算分为两个部分。
3.1.1 soft max分类器
交叉熵损失是基于softmax计算来的，softmax将网络最后输出z通过指数转变成概率形式。首先看一下softmax计算公式： 
其中， 分子
 是要计算的类别 
 的网络输出的指数；分母是所有类别网络输出的指数和，共k个类别。这样就得到了类别i的输出概率 
 。
→这里说点题外话，实际上，softmax是由逻辑斯的回归模型（用于二分类）推广得到的多项逻辑斯蒂回归模型（用于多分类）。具体可以参考李航大神的《统计学方法》第六章，这里给一个大致的过程。
逻辑回归的P(Y=y|x)表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：

将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：

逻辑回归最后得到的目标式子如下：

3.1.2 交叉熵损失
公式定义如下： 
其中， 
是类别 
的真实标签；
是上面softmax计算出的类别 
的概率值；k是类别数，N是样本总数。
这里看一个计算交叉熵损失的小例子：
假设共有三个类别cat、dog、bird，那么一张cat的图片标签应该为 
。并且训练过程中，这张cat的图片经过网络后得到三个类别网络的输出分别为3、1、-3。那么经过softmax可以得到对应的概率值，如下图：



3.2 交叉熵损失的两个图像
3.2.1 指数图像
softmax分类器将各个类别的“得分”（网络输出）转变成概率值。并取e指数使得“得分”高的类别对应的概率更大，使得损失函数对网络输出“更敏感”，更有利于分类。

3.2.2 对数图像

3.2.3 交叉熵损失+Sigmoid激活函数：
接着上一部分留下的问题，我们仍然以Sigmoid激活函数 
(其中 
)为例。这次我们引入交叉熵损失，并以二分类为例，那么s损失函数公式为：



那么可以计算一下最后一层的反向传播过程，可得：

根据之前的推导已知 
，那么上式可以化简为：



可以看到sigmoid的导数被约掉，这样最后一层的梯度中就没有
。然而这只是输出层的推导，如果变成隐藏层的梯度sigmoid的导数不会被约掉，仍然存在
。所以交叉熵损失+Sigmoid激活函数可以解决输出层神经元学习率缓慢的问题，但是不能解决隐藏层神经元学习率缓慢的问题。
其实损失函数包含两个部分：①计算方法（均方差、交叉熵等）②激活函数。
而之前我们遇到的是均方差损失+sigmoid激活函数造成了输出层神经元学习率缓慢，其实我们破坏任意一个条件都有可能解决这个问题：
①均方误差损失→交叉熵损失；
②sigmoid函数→不会造成梯度消失的函数，例如ReLU函数，不仅能解决输出层学习率缓慢，还能解决隐藏层学习率缓慢问题。

这里也小结一下ReLU函数相对于tanh和sigmoid函数好在哪里：
第一，采用sigmoid等函数，算激活函数是（指数运算），计算量大；反向传播求误差梯度时，求导涉及除法，计算量相对大。而采用Relu激活函数，整个过程的计算量节省很多。
第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0），这种情况会造成信息丢失，梯度消失在网络层数多的时候尤其明显，从而无法完成深层网络的训练。
第三，ReLU会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。
四、svm合页损失
4.1 定义
合页损失函数想让正确分类的“得分”比其他错误分类的“得分”高出至少一个边界值
。
如果正确分类的得分与错误分类的得分差值比边界值还要高，就会认为损失值是0；如果没有
就要计算损失了。看一下计算公式和示意图：

其中， 
是正确分类的得分、 
是其他错误分类的得分； 
是指想要正确类别的分类得分比其他错误分类类别的得分要高且至少高出 
的边界值；k是类别数，N是样本总数。
示意图如下：

这里看一个计算合页损失的小例子：仍然假设共有三个类别cat、dog、bird，那么一张cat的图片标签应该为 
 。并且训练过程中，这张cat的图片经过网络后得到三个类别网络的输出分别为3、1、-3。我们取 
 。此时： 
 。其实直观上也很好理解，分类正确的得分是3，其他错误类别得分是1和-3，而我们希望分类正确的得分比其他分类错误的得分高 
 的边界值。显然错误分类得分为1的没有符合条件，则计算损失。
4.2 特点
合页损失函数其实就是线性支持向量机中，对于一些线性不可分的数据，引入了松弛变量 
。这样，目标函数和约束条件就变成了：



其中前面的就是合页损失函数。后面的 
是正则项。
线性支持向量机也是希望不仅仅可以求出分类超平面，同时也希望正确分类比其他错误分类多出一个边界值，即分类间隔，SVM目的也就是最大化分类间隔。而引入的 
松弛因子其实就是计算的合页损失项。
尽管合页损失函数希望正确分类的得分比其他错误分类的得分高出至少一个边界值 
 ，但是
对于得分数字的细节是不关心的，看一个小例子：
如果两个分类器最后得分是[3,-10, -10]和[3,-2, -2]，且 
 ，那么对于合页损失来讲没什么不同，只要满足超过边界值5，那么损失值就都等于0。然而，显然第一个分类器比第二个分类器效果更好，因为高出边界更大，但是合页损失都是0，这就是合页损失对于得分数字的细节是不关心的造成的缺点。
想要解决这一问题，其实上面的交叉熵损失很好的解决这一问题，因为交叉熵将得分转变成概率，就不会造成上面说的情况；并且交叉熵损失也扩大了正确分类和错误分类得分的差距，对分数敏感，同样能得到较好的分类效果。
五、Smooth L1损失
Smooth L1损失是为了解决梯度爆炸问题的。在看Smooth L1损失之前，先看一下梯度爆炸。
1、梯度爆炸：
在深层神经网络或循环神经网络中，误差的梯度可在更新中累积相乘。如果网络层之间的梯度值大于 1.0，那么重复相乘会导致梯度呈指数级增长，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。
梯度爆炸会伴随一些细微的信号，如：
①模型不稳定，导致更新过程中的损失出现显著变化；
②训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。
2、Smooth L1损失：
Smooth L1损失函数是在Fast R-CNN中被提出，主要目的是为了防止梯度爆炸。
对于目标检测中的回归问题，最初大多采用均方误差损失 
 ，这样反向传播对w或者b求导时仍存在 
 。那么当预测值和目标值相差很大时，就容易造成梯度爆炸。
所以我们将 
 这种均方误差形式，转变成 
 这种形式，其中：

通过上式可以看出：
①当 
 时，即预测值和目标值相差小于1，不易造成梯度爆炸，此时还原成均方误差损失形式并给一个0.5的平滑系数，即 
 ；
②当 
 时，即预测值和目标值相差大于等于1，易造成梯度爆炸，此时降低损失次幂数，变成 
 ，这时候反向传播求导时候就不存在 
 这一项了，从而防止了梯度爆炸。
→这里最后再给出解决梯度爆炸的一些其他方法:
（1）减少学习率（个人理解梯度爆炸是模型训练发散的一种情况）；
（2）使用ReLU函数，使得梯度稳定；
（3）使用正则化，即检查网络中权重的大小，对较大的权重进行惩罚，限制了梯度爆炸造成的权重变得很大的情况。

来自 <https://blog.csdn.net/zhangjunp3/article/details/80467350?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase> 




1 什么是损失函数
机器学习中的损失函数（loss function）是用来评估模型的预测值-f(x)与真实值-y的不一致程度，损失函数越小，代表模型的鲁棒性越好，损失函数能指导模型学习。
 
2 分类任务损失 
2.1、0-1 loss
0-1 loss是最原始的loss，它直接比较输出值与输入值是否相等，对于样本i，它的loss等于：

当标签与预测类别相等时，loss为0，否则为1。可以看出，0-1 loss无法对x进行求导，在依赖于反向传播的深度学习任务中，无法被使用，0-1 loss更多的是启发新的loss的产生。
2.2、熵与交叉熵loss
物理学中的熵，表示一个热力学系统的无序程度。为了解决对信息的量化度量问题，香农在1948年提出了“信息熵”的概念，它使用对数函数表示对不确定性的测量。熵越高，表示能传输的信息越多，熵越少，表示传输的信息越少，可以直接将熵理解为信息量。
按照香农的理论，熵背后的原理是任何信息都存在冗余，并且冗余大小与信息中每个符号（数字、字母或单词）的出现概率或者说不确定性有关。概率大，出现机会多，则不确定性小，这个关系就用对数函数来表征。
为什么选择对数函数而不是其他函数呢？首先，不确定性必须是概率P的单调递降函数，假设一个系统中各个离散事件互不相关，要求其总的不确定性等于各自不确定性之和，对数函数是满足这个要求的。将不确定性f定义为log(1/p)=-log(p)，其中p是概率。
对于单个的信息源，信源的平均不确定性就是单个符号不确定性-logpi的统计平均值，信息熵的定义如下：

假设有两个概率分布p(x)和q(x)，其中p是已知的分布，q是未知的分布，则其交叉熵函数是两个分布的互信息，可以反应其相关程度。
至此，引出了分类任务中最常用的loss，即log loss，又名交叉熵loss，后统一称为交叉熵：

n对应于样本数量，m是类别数量，yij 表示第i个样本属于分类j的标签，它是0或者1。对于单分类任务，只有一个分类的标签非零。f(xij) 表示的是样本i预测为j分类的概率。loss的大小完全取决于分类为正确标签那一类的概率，当所有的样本都分类正确时，loss=0，否则大于0。
2.3、softmax loss及其变种
假如log loss中的f(xij)的表现形式是softmax概率的形式，那么交叉熵loss就是熟知的softmax with cross-entropy loss，简称softmax loss，所以说softmax loss只是交叉熵的一个特例。
softmax loss被广泛用于分类分割等任务，且发展出了很多的变种，有针对不平衡样本问题的weighted softmax loss，  focal loss，针对蒸馏学习的soft softmax loss，促进类内更加紧凑的L-softmax Loss等一系列的改进：
2.3.1、softmax loss
softmax loss实际上是由softmax和cross-entropy loss组合而成，两者放一起数值计算更加稳定。推到过程如下：
令z是softmax层的输入，f(z)是softmax的输出，则
单个像素i的softmax loss等于cross-entropy error如下:

 
展开上式：

在caffe实现中，z即bottom blob，l(y,z)是top blob，反向传播时，就是要根据top blob diff得到bottom blob diff，所以要得到 

下面求loss对z的第k个节点的梯度

可见，传给groundtruth label节点和非groundtruth label节点的梯度是不一样的。
原始的softmax loss非常优雅，简洁，被广泛用于分类问题。它的特点就是优化类间的距离非常棒，但是优化类内距离时比较弱。
2.3.2、weighted softmax loss
假如一个二分类问题，两类的样本数目差距非常之大。如边缘检测问题，边缘像素的重要性明显比非边缘像素大的重要性要大，此时可以针对性的对样本进行加权。

wc就是这个权重， c=0代表边缘像素，c=1代表非边缘像素，则可以令w0=1，w1=0.001，即加大边缘像素的权重。这个权重，还可以动态地计算让其自适应。
2.3.3、soft softmax loss

当T=1时，就是softmax的定义，当T>1，就称之为soft softmax，T越大，因为Zk产生的概率差异就会越小。作者提出这个是为了迁移学习，生成软标签，然后将软标签和硬标签同时用于新网络的学习。
为什么这么用，当训练好一个模型之后，模型为所有的误标签都分配了很小的概率；然而实际上对于不同的错误标签，其被分配的概率仍然可能存在数个量级的悬殊差距。这个差距，在softmax中直接就被忽略了，但这其实是一部分有用的信息。
作者先利用softmax loss训练获得一个大模型，然后基于大模型的softmax输出结果获取每一类的概率，将这个概率，作为小模型训练时soft target的label。
2.3.4、Large-Margin Softmax Loss / L-Softmax loss
上图显示的是不同softmax loss和L-Softmax loss学习到的cnn特征分布。第一列就是softmax，第2列是L-Softmax loss在参数m取不同值时的分布。通过可视化特征，可知学习到的类间的特征是比较明显的，但是类内比较散。而large-margin softmax loss则类内更加紧凑，怎么做到的呢？
loss的定义形式:
由于 zk 是全连接层的输出，所以可以写成形式如下:

将内积更具体的表现出来，就是：
对于二分类的情况，对于属于第1类的样本，希望：

若对它提出更高的要求？由于cos函数在0～PI区间是递减函数，将其改为：

其中m>=1，
在这个条件下，原始的softmax条件仍然得到满足。
下图，如果W1=W2，那么满足条件2，显然需要θ1与θ2之间的差距变得更大，原来的softmax的decision boundary只有一个，而现在类别1和类别2的decision boundary不相同，这样类间的距离进一步增加，类内更近紧凑。

更具体的定义如下：


 
L-Softmax loss中，m是一个控制距离的变量，它越大训练会变得越困难，因为类内不可能无限紧凑。作者是通过一个LargeMargin全连接层+softmax loss来共同实现。
2.3.5、L2-constrained softmax loss
将学习的特征x归一化。观测到好的正面的脸，特征的L2-norm大，而特征不明显的脸，其对应的特征L2-norm小，因此，作者提出这样的约束来增强特征的区分度：

 
上面式就是将其归一化到固定值α。实际训练的时候都不需要修改代码，只需要添加L2-norm层与scale层，如下图：
 
为什么要加这个scale层？NormFace指出了直接归一化权重和特征，会导致loss不能下降。因为就算是极端情况下，多类样本，令正样本|wx|=1取最大值，负样本|wx|=-1取最小值，这时候分类概率也是

 
当类别数n=10，p=0.45；n=1000，p=0.007。当类别数增加到1000类时，正样本最大的概率还不足0.01，而反向求导的时候，梯度=1-p，会导致一直传回去很大的loss。所以，有必要在后面加上scale层。
2.4、KL散度
Kullback和Leibler定义了KL散度用于估计两个分布的相似性，定义如下；

 
Dkl是非负的，只有当p与q处处相等时，才会等于0。上式也等价于

 
其中l(p,p)是分布p的熵，而l(p,q)就是p和q的交叉熵。假如p是一个已知的分布，则熵是一个常数，此时Dkl(p|q)与l(p,q)只有一个常数的差异，两者是等价的。但是，KL散度并不是一个对称的loss，即Dkl (p|q) != Dkl (q|p)，KL散度常被用于生成式的模型。
2.5、Hinge loss
Hinge loss主要用于支持向量机中，它的称呼来源于损失的形式，定义如下：
 
如果分类正确，loss=0，如果错误则为1-f(x)，所以它是一个分段不光滑的曲线。Hinge loss被用来解SVM问题中的间距最大化问题。
2.6、Exponential loss与Logistic loss
Exponential loss是一个指数形式的loss，特点是梯度比较大，主要用于Adaboost集成学习算法中，定义如下：
 

logistic loss取了Exponential loss的对数形式，它的定义如下：
 
logistic loss 梯度相对变化更加平缓。
此外，还有sigmoid cross_entropy_loss，可以被用于多标签分类任务或者不需要创建类间竞争机制的分类任务，在Mask RCNN中就使用了sigmoid cross_entropy_loss。

 
以上涵盖了大部分常用的分类任务损失，大部分都是对数的形式，这是由信息熵的定义和参数似然估计的本质决定的。
3 回归任务损失
在回归任务中，回归的结果是一些整数或者实数，并没有先验的概率密度分布，常使用的loss是L1 loss和L2 loss。
3.1、L1 loss
Mean absolute loss(MAE)也被称为L1 Loss，是以绝对误差作为距离：

由于L1 loss具有稀疏性，为了惩罚较大的值，因此常常将其作为正则项添加到其他loss中作为约束。L1 loss的最大问题是梯度在零点不平滑，会跳过极小值点。
3.2、L2 loss
Mean Squared Loss/ Quadratic Loss(MSE loss)也被称为L2 loss，或欧氏距离，它以误差的平方和作为距离：
 
L2 loss也常常作为正则项。当预测值与目标值相差很大时, 梯度容易爆炸（因为梯度里包含了x−t）。
3.3、L1 loss与L2 loss的改进
原始的L1 loss和L2 loss都有缺陷，如L1 loss的最大问题是梯度不平滑，而L2 loss的最大问题是容易梯度爆炸，所以研究者们对其提出了很多的改进。
在faster RCNN框架中，使用了smooth L1 loss来综合L1与L2 loss的优点，定义如下:
在x比较小时，上式等价于L2 loss，保持平滑。在x比较大时，上式等价于L1 loss，可以限制数值的大小。
为了增强L2 loss对噪声(离群点)的鲁棒性，研究者提出了Huber loss，定义如下：

Huber对于离群点非常的有效，它同时结合了L1与L2的优点，不过多出来了一个delta参数需要进行训练。除此之外还有Log-Cosh Loss等损失。
从上文可以看出，L1/L2各有优劣，设计一个通用的框架同时满足L1/L2损失的优点是研究重点，有如下：

3.4、感知损失perceptual loss
对于图像风格化，图像超分辨率重建等任务来说，早期都使用了图像像素空间的L2 loss，但是L2 loss与人眼感知的图像质量并不匹配，恢复出来的图像往往细节表现不好。现在的研究中，L2 loss逐步被人眼感知loss所取代。人眼感知loss也被称为perceptual loss（感知损失），它与MSE采用图像像素进行求差的不同之处在于所计算的空间不再是图像空间。
研究者们常使用VGG等网络的特征，令φ来表示损失网络，Cj表示网络的第j层，CjHjWj表示第j层的特征图的大小，感知损失的定义如下：

 
可以看出，perceptual loss与L2 loss具有同样的形式，只是计算的空间被转换到了特征空间。
4 生成对抗网络损失
生成对抗网络即Generative Adversarial Networks，简称GAN，于2014年以后兴起的无监督学习网络。原始的用于生成图片的GAN的损失函数包括了生成式模型和判别式模型两部分，如今GAN被用于各类任务，其他的各种损失也被引入，在此仅针对GAN的基本损失进行阐述。
4.1、GAN的基本损失
GAN是在生成模型和判别模型的相互博弈中进行迭代优化，它的优化目标如下：
 

从上式可以看出，包含两个部分：Ex∼pdata(x)[logD(x)]和Ez∼pz(z)[log(1−D(G(z)))]，要求最大化判别模型D对真实样本的概率估计，最小化判别模型D对生成的样本的概率估计，生成器G则要求最大化D(G(z))，即最大化判别模型对生成样本的误判，这里的loss是对数log的形式。
原始的GAN的损失使用了JS散度，两个分布之间越接近，它们的JS散度越小，但实际上这并不适合衡量生成数据分布和真实数据分布的距离，相关的分析请查阅相关文献。
4.2、-log D trick
Ian Goodfellow提出了-log D trick，使得生成器的损失不依赖于生成器G，即把生成器loss改成如下：

最小化目标存在两个严重的问题：第一是它同时要最小化生成分布与真实分布的KL散度，却又要最大化两者的JS散度，这是矛盾的会导致梯度不稳定。第二，因为KL散度不是对称的，导致此时loss不对称，对于正确样本误分和错误样本误分的惩罚是不一样的。第一种错误对应的是“生成器没能生成真实的样本”，即多样性差，惩罚微小；第二种错误对应的是“生成器生成了不真实的样本”，即准确性低，惩罚巨大。这样造成生成器生成多样性很差的样本，出现了常说的模式崩塌(collapse mode)问题。
4.3、Wasserstein GAN(简称wgan)等改进方案
wgan采用了Earth-Mover距离(EM距离)作为loss，它是在最优路径规划下的最小消耗，计算的是在联合分布γ下，样本对距离的期望值：

 
与原始的GAN的loss形式相比，wgan的生成器和判别器的loss不取log。wessertein距离相比KL散度和JS散度的优势在于：即使两个分布的支撑集没有重叠或者重叠非常少，仍然能够反映两个分布的远近。而JS散度在此情况下是常量，KL散度可能无意义。wgan有一些问题，wgan-gp改进了wgan连续性限制的条件，相关最新研究请查阅文献。
4.4、LS-GAN
LS-GAN即Least Squares Generative Adversarial Networks。它的原理部分可以一句话概括：使用了最小二乘损失函数代替了GAN的损失函数。这相当于最小化P和Q之间的Pearson卡方散度（divergence），属于f-divergence的一种，有效地缓解了GAN训练不稳定和生成图像质量差多样性不足的问题。提出者认为使用JS散度并不能拉近真实分布和生成分布之间的距离，使用最小二乘可以将图像的分布尽可能的接近决策边界，其损失函数定义如下：

以交叉熵作为损失，它的特点是会使得生成器不会再优化那些被判别器识别为真实图片的生成图片，即使这些生成图片距离判别器的决策边界仍然很远，也就是距真实数据比较远，这意味着生成器的生成图片质量并不高。而要想最小二乘损失比较小，则在混淆判别器的前提下还得让生成器把距离决策边界比较远的生成图片拉向决策边界，这就是LS-GAN的优势。
4.5、Loss-sensitive-GAN
在原始的GAN的损失函数后添加了一个约束项来直接限定GAN的建模能力，它的损失函数如下：

优化将通过最小化这个目标来得到一个“损失函数" (下文称之为L函数)。L函数在真实样本上越小越好，在生成的样本上越大越好。它是以真实样本x和生成样本的一个度量为各自L函数的目标间隔，把x和生成样本分开。好处是如果生成的样本和真实样本已经很接近，就不必要求他们的L函数有个固定间隔，因为生成的样本已经很好。这样就可以集中力量提高那些距离真实样本还很远，真实度不那么高的样本，能更合理地使用LS-GAN的建模能力，被称为“按需分配”。

来自 <https://www.cnblogs.com/jeshy/p/10629556.html> 



cross entropy，logistic loss 和 KL-divergence的关系和区别

adrianna_xy 2017-07-14 15:00:19  12594  收藏 6
展开
先给出结论：
cross entropy和KL-divergence作为目标函数效果是一样的，从数学上来说相差一个常数。
logistic loss 是cross entropy的一个特例
1. cross entropy和KL-divergence
假设两个概率分布p(x)p(x)和q(x)q(x)， H(p,q)H(p,q)为cross entropy，DKL(p|q)DKL(p|q)为 KL divergence。

交叉熵的定义：
H(p,q)=−∑xp(x)logq(x)
H(p,q)=−∑xp(x)log⁡q(x)
KL divergence的定义：
DKL(p|q)=∑xp(x)logp(x)q(x)
DKL(p|q)=∑xp(x)log⁡p(x)q(x)
推导：
DKL(p|q)=∑xp(x)logp(x)q(x)=∑x(p(x)logp(x)−p(x)logq(x))=−H(p)−∑xp(x)logq(x)=−H(p)+H(p,q)
DKL(p|q)=∑xp(x)log⁡p(x)q(x)=∑x(p(x)log⁡p(x)−p(x)log⁡q(x))=−H(p)−∑xp(x)log⁡q(x)=−H(p)+H(p,q)
也就是说，cross entropy也可以定义为：
H(p,q)=DKL(p|q)+H(p)
H(p,q)=DKL(p|q)+H(p)
直观来说，由于p(x)是已知的分布，H(p)是个常数，cross entropy和KL divergence之间相差一个常数。

2. logistic loss 和cross entropy
假设p∈{y,1−y}p∈{y,1−y} ，q∈{y^,1−y^}q∈{y^,1−y^}， cross entropy可以写为logistic loss：
H(p,q)=−∑xp(x)logq(x)=−ylogy^−(1−y)log(1−y^)
————————————————
版权声明：本文为CSDN博主「adrianna_xy」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/u012223913/java/article/details/75112246

